# Datawarehouse Project
## Project description
Sparkify is a music streaming startup with a growing user base and song database.

Their user activity and songs metadata data resides in json files in S3. The goal of the project is to build an ETL pipeline that will extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.

## Project Datasets
### Song Dataset:
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
### Log Dataset:
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

## Database schema design
### Fact Table
- songplays - records in log data associated with song plays i.e. records with page NextSong
 songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
### Dimension Tables
- users - users in the app
 user_id, first_name, last_name, gender, level
- songs - songs in music database
 song_id, title, artist_id, year, duration
- artists - artists in music database
 artist_id, name, location, latitude, longitude
- time - timestamps of records in songplays broken down into specific units
 start_time, hour, day, week, month, year, weekday
 
## Project Structure
1. **IaC.ipynb**: set up the needed infrastructure for the project.
2. **sql_queries.py**: contains all sql queries needed for the project.
3. **create_tables.py**: drops and creates tables.
4. **etl.py**: extract data from the files in S3, stage it in redshift, and finally store it in the dimensional tables.

## How to run
1. To run this project you will need to fill the following information, and save it as dwh.cfg in the project root folder.

```
[CLUSTER]
HOST= 
DB_NAME=dwh
DB_USER=dwhuser
DB_PASSWORD=Passw0rd
DB_PORT=5439

[IAM_ROLE]
ARN= 

[S3]
LOG_DATA='s3://udacity-dend/log_data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song_data'

[AWS]
KEY=
SECRET=

[DWH] 
DWH_CLUSTER_TYPE       =multi-node
DWH_NUM_NODES          =4
DWH_NODE_TYPE          =dc2.large
DWH_IAM_ROLE_NAME      =dwhRole
DWH_CLUSTER_IDENTIFIER =dwhCluster
DWH_DB                 =dwh
DWH_DB_USER            =dwhuser
DWH_DB_PASSWORD        =Passw0rd
DWH_PORT               =5439

```

2. Follow the steps in **IaC** notebook to set up the needed infrastructure for this project.

3. Run the create_tables script to set up the database staging and analytical tables.

`$ python create_tables.py`

4. Finally, run the etl script to extract data from the files in S3, stage it in redshift, and finally store it in the dimensional tables.

`$ python etl.py`
