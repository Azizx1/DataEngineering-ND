# Data Lake Project
## Project description
Sparkify is a music streaming startup with a growing user base and song database.

In this project, we'll build an ETL pipeline for a data lake hosted on S3. To complete the project, we will load data from S3, process the data into analytics tables using Spark, and load them back into S3. We'll deploy this Spark process on a cluster using AWS.

## Project Datasets
### Song Dataset:
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.
### Log Dataset:
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

## Database schema design
### Fact Table
- songplays - records in log data associated with song plays i.e. records with page NextSong
 songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
### Dimension Tables
- users - users in the app
 user_id, first_name, last_name, gender, level
- songs - songs in music database
 song_id, title, artist_id, year, duration
- artists - artists in music database
 artist_id, name, location, latitude, longitude
- time - timestamps of records in songplays broken down into specific units
 start_time, hour, day, week, month, year, weekday
 
## Project Structure
1. **etl.py**: reads data from S3, processes that data using Spark, and writes them back to S3.
2. **dl.cfg**: contains AWS credentials.

## How to run
Run the etl script to read data from S3, processes that data using Spark, and writes them back to S3.
`$ python etl.py`
